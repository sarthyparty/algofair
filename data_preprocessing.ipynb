{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "There is an error with discussion_questions.csv and discussion_answers.csv which causes almost half the lines to be removed. This error is caused because of incorrect formating in the csv since commas in the discussion contents are being recognized as delimiters.\n",
    "\n",
    "delete all `{\\\"dtdId\\\":\\\"discussion/1\\\",\\\"value\\\":\\\"<co-content>`\n",
    "\n",
    "delete all `</co-content>\\\"}`\n",
    "\n",
    "replace all `\\\\\\\" with \"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_qs = pd.read_csv(\"MTC508/discussion_questions.csv\")\n",
    "disc_qs_votes = pd.read_csv(\"MTC508/discussion_question_votes.csv\")\n",
    "disc_qs_followings = pd.read_csv(\"MTC508/discussion_question_followings.csv\")\n",
    "disc_ans = pd.read_csv(\"MTC508/discussion_answers.csv\")\n",
    "disc_ans_votes = pd.read_csv(\"MTC508/discussion_answer_votes.csv\")\n",
    "users = pd.read_csv(\"MTC508/users.csv\")\n",
    "target = pd.read_csv(\"MTC508/MTC508_roster_outputlabel_Jaeyoon.csv\")\n",
    "ethnicity = pd.read_csv(\"MTC508/MTC508_roster_output_label_Jaeyoon.csv\")\n",
    "survey = pd.read_csv(\"MTC508/MTC508_roster_output_label_race_gender_from_survey_Jaeyoon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select important column from the user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_user_columns = [\"umich_user_id\", \"country_cd\", \"reported_or_inferred_gender\"]\n",
    "selected_user_columns = [\"umich_user_id\", \"country_cd\", \"reported_or_inferred_gender\", \"educational_attainment\"]\n",
    "# selected_user_columns = [\"umich_user_id\"]\n",
    "\n",
    "df = pd.DataFrame(users)[selected_user_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>umich_user_id</th>\n",
       "      <th>country_cd</th>\n",
       "      <th>reported_or_inferred_gender</th>\n",
       "      <th>educational_attainment</th>\n",
       "      <th>gender_508</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d8494a39cae7ac5417d491df43f8296a285b3cd6</td>\n",
       "      <td>US</td>\n",
       "      <td>female</td>\n",
       "      <td>BACHELOR_DEGREE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9c3e969ace3d1278a5e3ad42fa80dfc20379aaf5</td>\n",
       "      <td>US</td>\n",
       "      <td>female</td>\n",
       "      <td>PROFESSIONAL_DEGREE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c91dab5c98bead9e683d15e9e2cc2c4584a8e626</td>\n",
       "      <td>US</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2246b3c08e2f3ee5d17f125ccc70baf3691667f9</td>\n",
       "      <td>US</td>\n",
       "      <td>female</td>\n",
       "      <td>BACHELOR_DEGREE</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1f8331ad9f0125b0fc85a1c4ff28f9c198b6fca2</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              umich_user_id country_cd  \\\n",
       "0  d8494a39cae7ac5417d491df43f8296a285b3cd6         US   \n",
       "1  9c3e969ace3d1278a5e3ad42fa80dfc20379aaf5         US   \n",
       "2  c91dab5c98bead9e683d15e9e2cc2c4584a8e626         US   \n",
       "3  2246b3c08e2f3ee5d17f125ccc70baf3691667f9         US   \n",
       "4  1f8331ad9f0125b0fc85a1c4ff28f9c198b6fca2         US   \n",
       "\n",
       "  reported_or_inferred_gender educational_attainment gender_508  \n",
       "0                      female        BACHELOR_DEGREE        NaN  \n",
       "1                      female    PROFESSIONAL_DEGREE        NaN  \n",
       "2                      female                    NaN        NaN  \n",
       "3                      female        BACHELOR_DEGREE     Female  \n",
       "4                         NaN                    NaN        NaN  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender = survey[['umich_user_id', 'gender_508']]\n",
    "\n",
    "df = pd.merge(df, gender, on='umich_user_id', how='left') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of discussion answers a user has posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_counts = disc_ans.groupby('umich_discussions_user_id').size().reset_index(name='answer_count')\n",
    "df = pd.merge(df, answer_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['answer_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average length of a discussion answer by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average answer length for a user\n",
    "def calculate_average_answer_length(user_id):\n",
    "    user_answers = disc_ans[disc_ans['umich_discussions_user_id'] == user_id]\n",
    "    # Handle the case where there are no answers for the user\n",
    "    if len(user_answers) == 0:\n",
    "        return 0  \n",
    "    total_length = user_answers['discussion_answer_content'].str.len().sum()\n",
    "    return total_length / len(user_answers)\n",
    "\n",
    "df['average_answer_length'] = df['umich_user_id'].apply(calculate_average_answer_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes given to discussion answers by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vote_counts = disc_ans_votes.groupby('umich_discussions_user_id')['discussion_answer_vote_value'].sum().reset_index(name='total_votes_given_answers')\n",
    "df = pd.merge(df, user_vote_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['total_votes_given_answers'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes received on discussion answers by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_vote_counts_received = disc_ans_votes.groupby('discussion_answer_id')['discussion_answer_vote_value'].sum().reset_index(name='total_votes_received_answers')\n",
    "discussion_answers_with_votes = pd.merge(disc_ans, answer_vote_counts_received, on='discussion_answer_id', how='left')\n",
    "discussion_answers_with_votes = discussion_answers_with_votes[['umich_discussions_user_id', 'total_votes_received_answers']]\n",
    "votes = pd.merge(df, discussion_answers_with_votes, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "votes.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "votes['total_votes_received_answers'].fillna(0, inplace=True)\n",
    "votes = votes.groupby('umich_user_id')['total_votes_received_answers'].sum()\n",
    "df = pd.merge(df, votes, on='umich_user_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of discussion questions a user has posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_counts = disc_qs.groupby('umich_discussions_user_id').size().reset_index(name='question_count')\n",
    "df = pd.merge(df, questions_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['question_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average length of a discussion question by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average answer length for a user\n",
    "def calculate_average_question_length(user_id):\n",
    "    user_questions = disc_qs[disc_qs['umich_discussions_user_id'] == user_id]\n",
    "    # Handle the case where there are no answers for the user\n",
    "    if len(user_questions) == 0:\n",
    "        return 0  \n",
    "    total_length = user_questions['discussion_question_details'].str.len().sum()\n",
    "    return total_length / len(user_questions)\n",
    "\n",
    "df['average_question_length'] = df['umich_user_id'].apply(calculate_average_question_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes given to discussion questions by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vote_counts = disc_qs_votes.groupby('umich_discussions_user_id')['discussion_question_vote_value'].sum().reset_index(name='total_votes_given_questions')\n",
    "df = pd.merge(df, user_vote_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['total_votes_given_questions'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes received on discussion questions by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vote_counts_received = disc_qs_votes.groupby('discussion_question_id')['discussion_question_vote_value'].sum().reset_index(name='total_votes_received_questions')\n",
    "discussion_questions_with_votes = pd.merge(disc_ans, question_vote_counts_received, on='discussion_question_id', how='left')\n",
    "discussion_questions_with_votes = discussion_questions_with_votes[['umich_discussions_user_id', 'total_votes_received_questions']]\n",
    "votes = pd.merge(df, discussion_questions_with_votes, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "votes.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "votes['total_votes_received_questions'].fillna(0, inplace=True)\n",
    "votes = votes.groupby('umich_user_id')['total_votes_received_questions'].sum()\n",
    "df = pd.merge(df, votes, on='umich_user_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of discussion questions a user is following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, disc_qs_followings, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "\n",
    "# Group by 'umich_discussions_user_id' and count the number of questions followed by each user\n",
    "user_question_counts = merged_df.groupby('umich_discussions_user_id')['discussion_question_following_active'].count().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "user_question_counts.columns = ['umich_user_id', 'total_questions_following']\n",
    "\n",
    "# Merge user_question_counts back into the final_df \n",
    "df = pd.merge(df, user_question_counts, on='umich_user_id', how='left')\n",
    "\n",
    "# Replace NaN values in the 'total_questions_following' column with 0\n",
    "df['total_questions_following'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.drop('id', axis=1, inplace=True)\n",
    "df = pd.get_dummies(df, columns=['country_cd', 'educational_attainment'])\n",
    "\n",
    "# onehot_encoded = pd.get_dummies(df['gender_508'], prefix='gender', dummy_na=True)\n",
    "# df = pd.concat([df, onehot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "intl_countries = ['country_cd_AE', 'country_cd_AG', 'country_cd_AT',\n",
    "       'country_cd_AU', 'country_cd_BD', 'country_cd_BE', 'country_cd_CA',\n",
    "       'country_cd_CN', 'country_cd_FR', 'country_cd_GB', 'country_cd_GR',\n",
    "       'country_cd_GT', 'country_cd_GY', 'country_cd_HK', 'country_cd_ID',\n",
    "       'country_cd_IL', 'country_cd_IN', 'country_cd_IQ', 'country_cd_IT',\n",
    "       'country_cd_JO', 'country_cd_JP', 'country_cd_KR', 'country_cd_KW',\n",
    "       'country_cd_KY', 'country_cd_LC', 'country_cd_MX', 'country_cd_NG',\n",
    "       'country_cd_NL', 'country_cd_NZ', 'country_cd_OM', 'country_cd_PK',\n",
    "       'country_cd_PL', 'country_cd_PT', 'country_cd_PY', 'country_cd_SG',\n",
    "       'country_cd_SL', 'country_cd_SX', 'country_cd_SZ', 'country_cd_TH',\n",
    "       'country_cd_UA', 'country_cd_VG', 'country_cd_VI',\n",
    "       'country_cd_ZA']\n",
    "df.drop(intl_countries, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'country_cd_US': 'US'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['intl'] = ~df['US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop('gender_Prefer not to say', axis=1, inplace=True)\n",
    "# df.rename(columns={'gender_Female': 'female', 'gender_Male' : 'male', 'gender_Non-binary / Third gender' : 'non-binary',  'gender_nan' : 'gender_na'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bachelor_obtained'] = df[['educational_attainment_BACHELOR_DEGREE',\n",
    "       'educational_attainment_DOCTORATE_DEGREE',\n",
    "       'educational_attainment_MASTERS_DEGREE',\n",
    "       'educational_attainment_PROFESSIONAL_DEGREE']].any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_bachelor_obtained'] = df[['educational_attainment_ASSOCIATE_DEGREE',\n",
    "       'educational_attainment_COLLEGE_NO_DEGREE',\n",
    "       'educational_attainment_HIGH_SCHOOL_DIPLOMA',\n",
    "       'educational_attainment_LESS_THAN_HIGH_SCHOOL_DIPLOMA']].any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"education_na\"] = np.where((df['bachelor_obtained'] == 0) & (df['no_bachelor_obtained'] == 0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['educational_attainment_ASSOCIATE_DEGREE',\n",
    "       'educational_attainment_BACHELOR_DEGREE',\n",
    "       'educational_attainment_COLLEGE_NO_DEGREE',\n",
    "       'educational_attainment_DOCTORATE_DEGREE',\n",
    "       'educational_attainment_HIGH_SCHOOL_DIPLOMA',\n",
    "       'educational_attainment_LESS_THAN_HIGH_SCHOOL_DIPLOMA',\n",
    "       'educational_attainment_MASTERS_DEGREE',\n",
    "       'educational_attainment_PROFESSIONAL_DEGREE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['Prefer not to say', 'Prefer to self-describe']\n",
    "\n",
    "# Use the logical OR operator to create the condition\n",
    "condition = (ethnicity[columns_to_check] == 1.0).any(axis=1)\n",
    "\n",
    "# Use boolean indexing to drop the rows that meet the condition\n",
    "ethnicity = ethnicity[~condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = \"White\"\n",
    "\n",
    "# count_ones = ethnicity[col].eq(1.0).sum()\n",
    "# count_zeros = ethnicity[col].eq(0.0).sum()\n",
    "# count_nan = ethnicity[col].isna().sum()\n",
    "# print(count_ones, count_zeros, count_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey['gender_508'].eq('Non-binary / Third gender').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"male\"] = (df[\"reported_or_inferred_gender\"] == \"male\") | (df[\"gender_508\"] == \"Male\")\n",
    "df[\"female\"] = (df[\"reported_or_inferred_gender\"] == \"female\") | (df[\"gender_508\"] == \"Female\")\n",
    "df[\"gender_other\"] = (df[\"gender_508\"] == \"Non-binary / Third gender\")\n",
    "df[\"gender_na\"] = (df[\"reported_or_inferred_gender\"].isna()) & (df[\"gender_508\"].isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"reported_or_inferred_gender\", \"gender_508\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnicity_drop_columns = ['Unnamed: 0',\n",
    "       'essentials_of_social_welfare_policy_user_id', 'completed',\n",
    "       'phoenix_session_user_id', 'outcome_label_completed',\n",
    "       'Prefer not to say', 'Prefer to self-describe']\n",
    "ethnicity.drop(ethnicity_drop_columns, axis=1, inplace=True)\n",
    "\n",
    "ethnicity[\"race_others\"] = ethnicity[['North American Indigenous', 'Hawaiian & Pacific Islander', 'Middle Eastern or North African']].any(axis=1).astype(int)\n",
    "race_columns = ['White', 'Black', 'Asian', 'Hispanic, Latino, or Spanish origin', 'North American Indigenous', 'Hawaiian & Pacific Islander', 'Middle Eastern or North African']\n",
    "ethnicity['race_na'] = ethnicity[race_columns].isna().all(axis=1).astype(int)\n",
    "\n",
    "ethnicity.drop(['North American Indigenous', 'Hawaiian & Pacific Islander', 'Middle Eastern or North African'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, ethnicity, on='umich_user_id', how='left')\n",
    "df = df.rename(columns={'Hispanic, Latino, or Spanish origin': 'latinx', 'Black': 'black', 'Asian': 'asian', 'White': 'white'})\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['US', 'intl', 'male', 'female', 'gender_other', 'gender_na', 'bachelor_obtained', 'no_bachelor_obtained', 'education_na', 'latinx', 'black', 'asian', 'white', 'race_others', 'race_na']] = df[['US', 'intl', 'male', 'female', 'gender_other', 'gender_na', 'bachelor_obtained', 'no_bachelor_obtained', 'education_na', 'latinx', 'black', 'asian', 'white', 'race_others', 'race_na']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target[['umich_user_id', 'completed']]\n",
    "df = pd.merge(df, target, on=\"umich_user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['female'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                        565.500000\n",
       "answer_count                        4.326855\n",
       "average_answer_length             287.235493\n",
       "total_votes_given_answers           0.730565\n",
       "total_votes_received_answers        0.786219\n",
       "question_count                      0.120141\n",
       "average_question_length            13.694562\n",
       "total_votes_given_questions         0.009717\n",
       "total_votes_received_questions      0.417845\n",
       "total_questions_following           0.060954\n",
       "US                                  0.889576\n",
       "intl                                0.110424\n",
       "bachelor_obtained                   0.458481\n",
       "no_bachelor_obtained                0.053887\n",
       "education_na                        0.487633\n",
       "male                                0.160777\n",
       "female                              0.670495\n",
       "gender_other                        0.004417\n",
       "gender_na                           0.166961\n",
       "white                               0.337456\n",
       "latinx                              0.042403\n",
       "black                               0.082155\n",
       "asian                               0.050353\n",
       "race_others                         0.016784\n",
       "race_na                             0.415194\n",
       "completed                           0.493816\n",
       "dtype: float64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/preprocessed.csv')\n",
    "# test['bachelor_obtained'].sum() + test['no_bachelor_obtained'].sum() + test['education_na'].sum()\n",
    "test.drop('umich_user_id', axis=1, inplace=True)\n",
    "test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = final_df.drop('completed', axis=1)\n",
    "# y = final_df['completed']\n",
    "\n",
    "# model = LogisticRegression()\n",
    "\n",
    "# scores = cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "# for i, score in enumerate(scores, 1):\n",
    "#     print(f'Fold {i}: Accuracy = {score:.2f}')\n",
    "\n",
    "\n",
    "# mean_accuracy = scores.mean()\n",
    "# std_accuracy = scores.std()\n",
    "# print(f'Mean Accuracy = {mean_accuracy:.2f}')\n",
    "# print(f'Standard Deviation = {std_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X, y)\n",
    "# coefficients = model.coef_\n",
    "# coefficients_dict = (np.std(X, 0)*(model.coef_[0])).to_dict()\n",
    "# # print(model.coef_)\n",
    "# # coefficients_dict = dict(zip(X.columns, coefficients[0]))\n",
    "# print(coefficients_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(coefficients_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(coefficients_dict.items()), key=lambda x : abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X-np.mean(X, axis=0)\n",
    "# X = X/np.std(X, axis=0)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# # for i, score in enumerate(scores, 1):\n",
    "# #     print(f'Fold {i}: Accuracy = {score:.2f}')\n",
    "\n",
    "# # mean_accuracy = scores.mean()\n",
    "# # std_accuracy = scores.std()\n",
    "\n",
    "# # print(f'Mean Accuracy = {mean_accuracy:.2f}')\n",
    "# # print(f'Standard Deviation = {std_accuracy:.2f}')\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# # Print the AUC score\n",
    "# print(f'AUC: {auc:.2f}')\n",
    "\n",
    "# coefficients = model.coef_\n",
    "# coefficients_dict = (np.std(X, 0)*(model.coef_[0])).to_dict()\n",
    "\n",
    "# sorted(list(coefficients_dict.items()), key=lambda x : abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['educational_attainment_BACHELOR_DEGREE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # Extract FP and TN from the confusion matrix\n",
    "# FP = conf_matrix[0, 1]  # False Positives\n",
    "# TN = conf_matrix[1, 1]  # True Negatives\n",
    "\n",
    "# # Calculate FPR\n",
    "# FPR = FP / (FP + TN)\n",
    "# print(\"False Positive Rate:\", FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop('completed', axis=1)\n",
    "# y = df['completed']\n",
    "\n",
    "# model = LogisticRegression()\n",
    "\n",
    "# kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# aucs = []\n",
    "# fprs = []\n",
    "\n",
    "# # print(X.shape, y.shape)\n",
    "# # print(y)\n",
    "\n",
    "# y = np.array(y)\n",
    "# X = np.array(X)\n",
    "# for train_index, test_index in kfold.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     auc = roc_auc_score(y_test, y_pred)\n",
    "#     aucs.append(auc)\n",
    "\n",
    "#     # Print the AUC score\n",
    "#     # print(f'AUC: {auc:.2f}')\n",
    "    \n",
    "#     # Train and evaluate your model on the current fold\n",
    "#     # You can replace this with your model training and evaluation code\n",
    "#     conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#     # Extract FP and TN from the confusion matrix\n",
    "#     FP = conf_matrix[0, 1]  # False Positives\n",
    "#     TN = conf_matrix[1, 1]  # True Negatives\n",
    "\n",
    "#     # Calculate FPR\n",
    "#     FPR = FP / (FP + TN)\n",
    "#     fprs.append(FPR)\n",
    "#     # print(\"False Positive Rate:\", FPR)\n",
    "\n",
    "# auc_avg = np.average(aucs)\n",
    "# auc_std = np.std(aucs)\n",
    "# fpr_avg = np.average(fprs)\n",
    "# fpr_std = np.std(fprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'AUC: {auc_avg:.2f} +/- {auc_std:.2f}')\n",
    "# print(f'FPR: {fpr_avg:.2f} +/- {fpr_std:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
