{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "There is an error with discussion_questions.csv and discussion_answers.csv which causes almost half the lines to be removed. This error is caused because of incorrect formating in the csv since commas in the discussion contents are being recognized as delimiters.\n",
    "\n",
    "delete all `{\\\"dtdId\\\":\\\"discussion/1\\\",\\\"value\\\":\\\"<co-content>`\n",
    "\n",
    "delete all `</co-content>\\\"}`\n",
    "\n",
    "replace all `\\\\\\\" with \"\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_qs = pd.read_csv(\"MTC508/discussion_questions.csv\")\n",
    "disc_qs_votes = pd.read_csv(\"MTC508/discussion_question_votes.csv\")\n",
    "disc_qs_followings = pd.read_csv(\"MTC508/discussion_question_followings.csv\")\n",
    "disc_ans = pd.read_csv(\"MTC508/discussion_answers.csv\")\n",
    "disc_ans_votes = pd.read_csv(\"MTC508/discussion_answer_votes.csv\")\n",
    "users = pd.read_csv(\"MTC508/users.csv\")\n",
    "target = pd.read_csv(\"MTC508/MTC508_roster_outputlabel_Jaeyoon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select important column from the user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_user_columns = [\"umich_user_id\", \"country_cd\", \"reported_or_inferred_gender\"]\n",
    "selected_user_columns = [\"umich_user_id\", \"country_cd\", \"reported_or_inferred_gender\",\"educational_attainment\"]\n",
    "# selected_user_columns = [\"umich_user_id\"]\n",
    "\n",
    "df = pd.DataFrame(users)[selected_user_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of discussion answers a user has posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_counts = disc_ans.groupby('umich_discussions_user_id').size().reset_index(name='answer_count')\n",
    "df = pd.merge(df, answer_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['answer_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average length of a discussion answer by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average answer length for a user\n",
    "def calculate_average_answer_length(user_id):\n",
    "    user_answers = disc_ans[disc_ans['umich_discussions_user_id'] == user_id]\n",
    "    # Handle the case where there are no answers for the user\n",
    "    if len(user_answers) == 0:\n",
    "        return 0  \n",
    "    total_length = user_answers['discussion_answer_content'].str.len().sum()\n",
    "    return total_length / len(user_answers)\n",
    "\n",
    "df['average_answer_length'] = df['umich_user_id'].apply(calculate_average_answer_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes given to discussion answers by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vote_counts = disc_ans_votes.groupby('umich_discussions_user_id')['discussion_answer_vote_value'].sum().reset_index(name='total_votes_given_answers')\n",
    "df = pd.merge(df, user_vote_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['total_votes_given_answers'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes received on discussion answers by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_vote_counts_received = disc_ans_votes.groupby('discussion_answer_id')['discussion_answer_vote_value'].sum().reset_index(name='total_votes_received_answers')\n",
    "discussion_answers_with_votes = pd.merge(disc_ans, answer_vote_counts_received, on='discussion_answer_id', how='left')\n",
    "discussion_answers_with_votes = discussion_answers_with_votes[['umich_discussions_user_id', 'total_votes_received_answers']]\n",
    "votes = pd.merge(df, discussion_answers_with_votes, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "votes.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "votes['total_votes_received_answers'].fillna(0, inplace=True)\n",
    "votes = votes.groupby('umich_user_id')['total_votes_received_answers'].sum()\n",
    "df = pd.merge(df, votes, on='umich_user_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of discussion questions a user has posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_counts = disc_qs.groupby('umich_discussions_user_id').size().reset_index(name='question_count')\n",
    "df = pd.merge(df, questions_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['question_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average length of a discussion question by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average answer length for a user\n",
    "def calculate_average_question_length(user_id):\n",
    "    user_questions = disc_qs[disc_qs['umich_discussions_user_id'] == user_id]\n",
    "    # Handle the case where there are no answers for the user\n",
    "    if len(user_questions) == 0:\n",
    "        return 0  \n",
    "    total_length = user_questions['discussion_question_details'].str.len().sum()\n",
    "    return total_length / len(user_questions)\n",
    "\n",
    "df['average_question_length'] = df['umich_user_id'].apply(calculate_average_answer_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes given to discussion questions by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vote_counts = disc_qs_votes.groupby('umich_discussions_user_id')['discussion_question_vote_value'].sum().reset_index(name='total_votes_given_questions')\n",
    "df = pd.merge(df, user_vote_counts, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "df.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "df['total_votes_given_questions'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the total number of votes received on discussion questions by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vote_counts_received = disc_qs_votes.groupby('discussion_question_id')['discussion_question_vote_value'].sum().reset_index(name='total_votes_received_questions')\n",
    "discussion_questions_with_votes = pd.merge(disc_ans, question_vote_counts_received, on='discussion_question_id', how='left')\n",
    "discussion_questions_with_votes = discussion_questions_with_votes[['umich_discussions_user_id', 'total_votes_received_questions']]\n",
    "votes = pd.merge(df, discussion_questions_with_votes, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "votes.drop('umich_discussions_user_id', axis=1, inplace=True)\n",
    "votes['total_votes_received_questions'].fillna(0, inplace=True)\n",
    "votes = votes.groupby('umich_user_id')['total_votes_received_questions'].sum()\n",
    "df = pd.merge(df, votes, on='umich_user_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of discussion questions a user is following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df, disc_qs_followings, left_on='umich_user_id', right_on='umich_discussions_user_id', how='left')\n",
    "\n",
    "# Group by 'umich_discussions_user_id' and count the number of questions followed by each user\n",
    "user_question_counts = merged_df.groupby('umich_discussions_user_id')['discussion_question_following_active'].count().reset_index()\n",
    "\n",
    "# Rename the columns\n",
    "user_question_counts.columns = ['umich_user_id', 'total_questions_following']\n",
    "\n",
    "# Merge user_question_counts back into the final_df \n",
    "df = pd.merge(df, user_question_counts, on='umich_user_id', how='left')\n",
    "\n",
    "# Replace NaN values in the 'total_questions_following' column with 0\n",
    "df['total_questions_following'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.drop('id', axis=1, inplace=True)\n",
    "df = pd.get_dummies(df, columns=['country_cd', 'reported_or_inferred_gender', 'educational_attainment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "intl_countries = ['country_cd_AE', 'country_cd_AG', 'country_cd_AT',\n",
    "       'country_cd_AU', 'country_cd_BD', 'country_cd_BE', 'country_cd_CA',\n",
    "       'country_cd_CN', 'country_cd_FR', 'country_cd_GB', 'country_cd_GR',\n",
    "       'country_cd_GT', 'country_cd_GY', 'country_cd_HK', 'country_cd_ID',\n",
    "       'country_cd_IL', 'country_cd_IN', 'country_cd_IQ', 'country_cd_IT',\n",
    "       'country_cd_JO', 'country_cd_JP', 'country_cd_KR', 'country_cd_KW',\n",
    "       'country_cd_KY', 'country_cd_LC', 'country_cd_MX', 'country_cd_NG',\n",
    "       'country_cd_NL', 'country_cd_NZ', 'country_cd_OM', 'country_cd_PK',\n",
    "       'country_cd_PL', 'country_cd_PT', 'country_cd_PY', 'country_cd_SG',\n",
    "       'country_cd_SL', 'country_cd_SX', 'country_cd_SZ', 'country_cd_TH',\n",
    "       'country_cd_UA', 'country_cd_VG', 'country_cd_VI',\n",
    "       'country_cd_ZA']\n",
    "df.drop(intl_countries, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'reported_or_inferred_gender_male': 'is_male'}, inplace=True)\n",
    "df.drop('reported_or_inferred_gender_female', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bachelor_obtained'] = df[['educational_attainment_BACHELOR_DEGREE',\n",
    "       'educational_attainment_DOCTORATE_DEGREE',\n",
    "       'educational_attainment_MASTERS_DEGREE',\n",
    "       'educational_attainment_PROFESSIONAL_DEGREE']].any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['educational_attainment_ASSOCIATE_DEGREE',\n",
    "       'educational_attainment_BACHELOR_DEGREE',\n",
    "       'educational_attainment_COLLEGE_NO_DEGREE',\n",
    "       'educational_attainment_DOCTORATE_DEGREE',\n",
    "       'educational_attainment_HIGH_SCHOOL_DIPLOMA',\n",
    "       'educational_attainment_LESS_THAN_HIGH_SCHOOL_DIPLOMA',\n",
    "       'educational_attainment_MASTERS_DEGREE',\n",
    "       'educational_attainment_PROFESSIONAL_DEGREE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['country_cd_US', 'is_male', 'bachelor_obtained']] = df[['country_cd_US', 'is_male', 'bachelor_obtained']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, target, on='umich_user_id', how='left')\n",
    "df.drop('essentials_of_social_welfare_policy_user_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = final_df.drop('completed', axis=1)\n",
    "# y = final_df['completed']\n",
    "\n",
    "# model = LogisticRegression()\n",
    "\n",
    "# scores = cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "# for i, score in enumerate(scores, 1):\n",
    "#     print(f'Fold {i}: Accuracy = {score:.2f}')\n",
    "\n",
    "\n",
    "# mean_accuracy = scores.mean()\n",
    "# std_accuracy = scores.std()\n",
    "# print(f'Mean Accuracy = {mean_accuracy:.2f}')\n",
    "# print(f'Standard Deviation = {std_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X, y)\n",
    "# coefficients = model.coef_\n",
    "# coefficients_dict = (np.std(X, 0)*(model.coef_[0])).to_dict()\n",
    "# # print(model.coef_)\n",
    "# # coefficients_dict = dict(zip(X.columns, coefficients[0]))\n",
    "# print(coefficients_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(coefficients_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(list(coefficients_dict.items()), key=lambda x : abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X-np.mean(X, axis=0)\n",
    "# X = X/np.std(X, axis=0)\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = cross_val_score(model, X, y, cv=10)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# # for i, score in enumerate(scores, 1):\n",
    "# #     print(f'Fold {i}: Accuracy = {score:.2f}')\n",
    "\n",
    "# # mean_accuracy = scores.mean()\n",
    "# # std_accuracy = scores.std()\n",
    "\n",
    "# # print(f'Mean Accuracy = {mean_accuracy:.2f}')\n",
    "# # print(f'Standard Deviation = {std_accuracy:.2f}')\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# # Print the AUC score\n",
    "# print(f'AUC: {auc:.2f}')\n",
    "\n",
    "# coefficients = model.coef_\n",
    "# coefficients_dict = (np.std(X, 0)*(model.coef_[0])).to_dict()\n",
    "\n",
    "# sorted(list(coefficients_dict.items()), key=lambda x : abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['educational_attainment_BACHELOR_DEGREE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # Extract FP and TN from the confusion matrix\n",
    "# FP = conf_matrix[0, 1]  # False Positives\n",
    "# TN = conf_matrix[1, 1]  # True Negatives\n",
    "\n",
    "# # Calculate FPR\n",
    "# FPR = FP / (FP + TN)\n",
    "# print(\"False Positive Rate:\", FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop('completed', axis=1)\n",
    "# y = df['completed']\n",
    "\n",
    "# model = LogisticRegression()\n",
    "\n",
    "# kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# aucs = []\n",
    "# fprs = []\n",
    "\n",
    "# # print(X.shape, y.shape)\n",
    "# # print(y)\n",
    "\n",
    "# y = np.array(y)\n",
    "# X = np.array(X)\n",
    "# for train_index, test_index in kfold.split(X):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     auc = roc_auc_score(y_test, y_pred)\n",
    "#     aucs.append(auc)\n",
    "\n",
    "#     # Print the AUC score\n",
    "#     # print(f'AUC: {auc:.2f}')\n",
    "    \n",
    "#     # Train and evaluate your model on the current fold\n",
    "#     # You can replace this with your model training and evaluation code\n",
    "#     conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#     # Extract FP and TN from the confusion matrix\n",
    "#     FP = conf_matrix[0, 1]  # False Positives\n",
    "#     TN = conf_matrix[1, 1]  # True Negatives\n",
    "\n",
    "#     # Calculate FPR\n",
    "#     FPR = FP / (FP + TN)\n",
    "#     fprs.append(FPR)\n",
    "#     # print(\"False Positive Rate:\", FPR)\n",
    "\n",
    "# auc_avg = np.average(aucs)\n",
    "# auc_std = np.std(aucs)\n",
    "# fpr_avg = np.average(fprs)\n",
    "# fpr_std = np.std(fprs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'AUC: {auc_avg:.2f} +/- {auc_std:.2f}')\n",
    "# print(f'FPR: {fpr_avg:.2f} +/- {fpr_std:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
